{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10316d5779a3733",
   "metadata": {
    "collapsed": false,
    "id": "10316d5779a3733",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Exercise 1: t-SNE\n",
    "\n",
    "## Do not start the exercise until you fully understand the submission guidelines.\n",
    "\n",
    "\n",
    "* The homework assignments are executed automatically.\n",
    "* Failure to comply with the following instructions will result in a significant penalty.\n",
    "* Appeals regarding your failure to read these instructions will be denied.\n",
    "\n",
    "## Read the following instructions carefully:\n",
    "\n",
    "1. This Jupyter notebook contains all the step-by-step instructions needed for this exercise.\n",
    "1. Write **efficient**, **vectorized** code whenever possible. Some calculations in this exercise may take several minutes when implemented efficiently, and might take much longer otherwise. Unnecessary loops will result in point deductions.\n",
    "1. You are responsible for the correctness of your code and should add as many tests as you see fit to this jupyter notebook. Tests will not be graded nor checked.\n",
    "1. You are allowed to use functions and methods from the [Python Standard Library](https://docs.python.org/3/library/).\n",
    "1. Your code must run without errors. Use at least `numpy` 1.15.4. Any code that cannot run will not be graded.\n",
    "1. Write your own code. Cheating will not be tolerated.\n",
    "1. Submission includes a zip file that contains this notebook, with your ID as the file name. For example, `hw1_123456789_987654321.zip` if you submitted in pairs and `hw1_123456789.zip` if you submitted the exercise alone. The name of the notebook should follow the same structure.\n",
    "   \n",
    "Please use only a **zip** file in your submission.\n",
    "\n",
    "---\n",
    "##‚ùó‚ùó‚ùó‚ùó‚ùó‚ùó‚ùó‚ùó‚ùó**This is mandatory**‚ùó‚ùó‚ùó‚ùó‚ùó‚ùó‚ùó‚ùó‚ùó\n",
    "## Please write your RUNI emails in this cell:\n",
    "\n",
    "### ***yonatan.greenshpan@post.runi.ac.il***\n",
    "---\n",
    "\n",
    "## Please sign that you have read and understood the instructions:\n",
    "\n",
    "### ***204266191***  \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b03f54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa929071",
   "metadata": {},
   "outputs": [],
   "source": [
    "# My additional libraries\n",
    "import pytest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f877cca-f7d1-41ca-9d3d-28d587bec85c",
   "metadata": {
    "id": "8f877cca-f7d1-41ca-9d3d-28d587bec85c",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Design your algorithm\n",
    "Make sure to describe the algorithm, its limitations, and describe use-cases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba7760b-b4ef-45a9-9aad-88b45fd8d442",
   "metadata": {
    "id": "0ba7760b-b4ef-45a9-9aad-88b45fd8d442"
   },
   "source": [
    "## **Algorithm Description**\n",
    "\n",
    "t-SNE is a nonlinear dimensionality-reduction algorithm designed to embed high-dimensional data into a low-dimensional space while preserving **local neighborhood structure**.\n",
    "\n",
    "The core idea is to convert pairwise distances in high-dimensional space into probability distributions, and then find a low-dimensional embedding whose probability distribution is as similar as possible by minimizing KL-divergence. It is importent to note, that in t-SNE we tune the data‚Äôs low-dimensional coordinates themselves rather than adjusting model parameters.\n",
    "\n",
    "The algorithm consists of the following stages:\n",
    "\n",
    "### **1. Compute Pairwise Affinities in High Dimension**\n",
    "\n",
    "For each point $x_i$, define conditional probabilities:\n",
    "\n",
    "$$\n",
    "p_{j|i} \\propto \\exp\\left(-\\frac{\\|x_i - x_j\\|^2}{2\\sigma_i^2}\\right)\n",
    "$$\n",
    "\n",
    "- $x_i$ ‚Äî the $i$-th data point in the original high-dimensional space.  \n",
    "- $x_j$ ‚Äî a neighboring data point whose similarity to $x_i$ we measure.  \n",
    "- $\\|x_i - x_j\\|^2$ ‚Äî squared Euclidean distance between points $x_i$ and $x_j$.  \n",
    "- $\\sigma_i$ ‚Äî the bandwidth (standard deviation) of the Gaussian centered at $x_i$, selected individually per point.  \n",
    "- $p_{j|i}$ ‚Äî the conditional probability that $x_i$ would pick $x_j$ as a neighbor.  \n",
    "\n",
    "### **2. Symmetrize the Probabilities**\n",
    "\n",
    "Because each point $x_i$ uses its own $\\sigma_i$, the conditional probabilities $p_{j|i}$ and $p_{i|j}$ reflect two different local neighborhoods (‚Äútwo different families of relatives‚Äù). So even if the distance $\\|x_i - x_j\\|$ is the same in both directions, the probabilities are not. t-SNE cannot work with two different notions of similarity for the same pair. To define **one shared, mutual similarity** between $x_i$ and $x_j$, we combine the two directional probabilities:  \n",
    "\n",
    "$$\n",
    "p_{ij} = \\frac{p_{j|i} + p_{i|j}}{2n}\n",
    "$$\n",
    "\n",
    "### **3. Initialize Low-Dimensional Embeddings**\n",
    "\n",
    "Randomly initialize $y_i \\in \\mathbb{R}^2$ (or $\\mathbb{R}^3$) according to the visualization type we want.\n",
    "\n",
    "\n",
    "### **4. Define Low-Dimensional Similarities Using a t-Distribution**\n",
    "\n",
    "In the low-dimensional space, we also want a *imilarity distribution between points: pairs that are close in 2D should get high similarity, and far pairs should get low similarity.\n",
    "\n",
    "If we used a **Gaussian** here (like in the high-dimensional space), many points would be pulled too close together in the center, which leads to the **crowding problem**: too many moderately distant points all collapse near the origin.\n",
    "\n",
    "To avoid this, t-SNE uses a **heavy-tailed** distribution so that moderately far points still exert noticeable ‚Äúrepulsive‚Äù force.  We choose a **Student-t distribution with 1 degree of freedom** (also known as the Cauchy distribution), whose probability density function is:\n",
    "\n",
    "$$\n",
    "f(t) = \\frac{\\Gamma\\!\\left(\\frac{\\nu + 1}{2}\\right)}\n",
    "{\\sqrt{\\nu\\pi}\\,\\Gamma\\!\\left(\\frac{\\nu}{2}\\right)}\n",
    "\\left(1 + \\frac{t^2}{\\nu}\\right)^{-\\frac{\\nu + 1}{2}}\n",
    "$$\n",
    "\n",
    "For $\\nu = 1$ this simplifies to (up to a constant factor):\n",
    "\n",
    "$$\n",
    "f(t) \\propto \\frac{1}{1 + t^2}\n",
    "$$\n",
    "\n",
    "In t-SNE we plug in the low-dimensional distance\n",
    "$t = \\|y_i - y_j\\|$ and then normalize over all pairs to get a proper probability distribution:\n",
    "\n",
    "$$\n",
    "q_{ij} = \\frac{(1 + \\|y_i - y_j\\|^2)^{-1}}\n",
    "{\\sum_{k \\neq l} (1 + \\|y_k - y_l\\|^2)^{-1}}\n",
    "$$\n",
    "\n",
    "This $q_{ij}$ is the **low-dimensional similarity** between $y_i$ and $y_j$.  \n",
    "\n",
    "\n",
    "### **5. Minimize KL-Divergence Between $P$ and $Q$**\n",
    "\n",
    "We optimize this expression iteratively with the reugular techniques:\n",
    "\n",
    "$$\n",
    "\\mathrm{KL}(P\\|Q)\n",
    "=\n",
    "\\sum_{i \\neq j} p_{ij} \\log \\frac{p_{ij}}{q_{ij}}\n",
    "$$\n",
    "\n",
    "## **Limitations**\n",
    "\n",
    "- Does **not scale well** to very large datasets, naively complexity is $O(n^2)$)\n",
    "- **Global structure is unreliable** ‚Äî only local neighborhoods are meaningful.\n",
    "- t-SNE optimizes the embedded coordinates themselves, no so we **don't get at the end of the learning a function that can map new data** into the space like in PCA\n",
    "- **Sensitive to hyperparameters** such as perplexity and learning rate.  \n",
    "- **No inverse transform** ‚Äî cannot reconstruct high-dimensional vectors from the 2D embedding.\n",
    "\n",
    "---\n",
    "\n",
    "## **Use-Cases**\n",
    "\n",
    "- **Visualization of high-dimensional datasets** such as images, text embeddings, biological data, user behavior features, or any complex structured data.\n",
    "- **Exploratory Data Analysis (EDA)** to reveal clusters, subgroups, anomalies, or hidden structure that may not be visible in the raw high-dimensional space.\n",
    "- **Understanding model representations**, for example examining the latent space of neural networks, autoencoders, or transformer embeddings.\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2158ff2629daf2bb",
   "metadata": {
    "collapsed": false,
    "id": "2158ff2629daf2bb",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Your implementations\n",
    "You may add new cells, write helper functions or test code as you see fit.\n",
    "Please use the cell below and include a description of your implementation.\n",
    "Explain code design consideration, algorithmic choices and any other details you think is relevant to understanding your implementation.\n",
    "Failing to explain your code will lead to point deductions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe206c9-d1f4-440b-aca0-c807cdd79451",
   "metadata": {
    "id": "afe206c9-d1f4-440b-aca0-c807cdd79451"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a1dfe42",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTSNE:\n",
    "    \"\"\"A from-scratch implementation of t-SNE with basic transform support.\"\"\"\n",
    "\n",
    "    def __init__(self, perplexity=30.0, n_components=2, n_iter=1000, learning_rate=200.0):\n",
    "        self.perplexity = perplexity\n",
    "        self.n_components = n_components\n",
    "        self.n_iter = n_iter\n",
    "        self.learning_rate = learning_rate\n",
    "        self.sigmas_ = None\n",
    "        self.X_fit_ = None\n",
    "        self.Y_fit_ = None\n",
    "\n",
    "    # ==========================================================================\n",
    "    # PUBLIC METHODS\n",
    "    # ==========================================================================\n",
    "\n",
    "    def fit_transform(self, X):\n",
    "        \"\"\"Fit t-SNE model to X and return a low-dimensional embedding.\"\"\"\n",
    "        X = np.asarray(X, dtype=float)\n",
    "        if X.ndim != 2:\n",
    "            raise ValueError(\"X must be a 2D array\")\n",
    "\n",
    "        P, sigmas = self._compute_high_dim_affinities(X)\n",
    "        Y_init = self._initialize_embedding(len(X), random_state=42)\n",
    "        Y = self._optimize_embedding(P, Y_init)\n",
    "\n",
    "        self.X_fit_ = X\n",
    "        self.sigmas_ = sigmas\n",
    "        self.Y_fit_ = Y\n",
    "        return Y\n",
    "\n",
    "    def transform(self, X_original, Y_original, X_new):\n",
    "        \"\"\"Map new samples into an existing t-SNE embedding.\"\"\"\n",
    "        X_original = np.asarray(X_original, dtype=float)\n",
    "        Y_original = np.asarray(Y_original, dtype=float)\n",
    "        X_new = np.asarray(X_new, dtype=float)\n",
    "\n",
    "        if X_original.shape[0] != Y_original.shape[0]:\n",
    "            raise ValueError(\"X_original and Y_original must contain the same number of samples\")\n",
    "        if X_original.shape[1] != X_new.shape[1]:\n",
    "            raise ValueError(\"Original and new data must have the same number of features\")\n",
    "\n",
    "        P_new = self._compute_conditional_probs_new(X_original, X_new)\n",
    "        Y_new_init = np.random.normal(loc=0.0, scale=1e-4, size=(X_new.shape[0], self.n_components))\n",
    "        return self._optimize_new_point_embedding(Y_original, P_new, Y_new_init)\n",
    "\n",
    "    # ==========================================================================\n",
    "    # PRIVATE METHODS\n",
    "    # ==========================================================================\n",
    "\n",
    "    def _compute_pairwise_distances(self, X):\n",
    "        \"\"\"Compute pairwise squared Euclidean distances between points in X.\"\"\"\n",
    "        X = np.asarray(X, dtype=float)\n",
    "        sum_sq = np.sum(X ** 2, axis=1, keepdims=True)\n",
    "        D = sum_sq + sum_sq.T - 2 * np.dot(X, X.T)\n",
    "        np.maximum(D, 0, out=D)\n",
    "        return D\n",
    "\n",
    "    def _compute_entropy_and_conditional_probs(self, Di, sigma):\n",
    "        \"\"\"Return entropy and conditional probabilities for row Di with bandwidth sigma.\"\"\"\n",
    "        sigma = max(float(sigma), 1e-10)\n",
    "        denom = 2.0 * sigma ** 2\n",
    "        Pi = np.exp(-Di / denom)\n",
    "        Pi[Di == 0] = 0.0  # exclude self\n",
    "        sum_pi = np.sum(Pi)\n",
    "        if sum_pi == 0:\n",
    "            Pi = np.zeros_like(Di)\n",
    "        else:\n",
    "            Pi /= sum_pi\n",
    "        Pi = np.clip(Pi, 1e-12, None)\n",
    "        Pi /= np.sum(Pi)\n",
    "        nz = Pi > 0\n",
    "        H = -np.sum(Pi[nz] * np.log2(Pi[nz]))\n",
    "        return H, Pi\n",
    "\n",
    "    def _binary_search_sigma(self, Di, desired_perplexity, tol=1e-5, max_iter=50):\n",
    "        \"\"\"Binary search sigma so that perplexity matches the target.\"\"\"\n",
    "        target_entropy = np.log2(desired_perplexity)\n",
    "        sigma_min, sigma_max = 1e-5, np.inf\n",
    "        sigma = 1.0\n",
    "        Pi = np.zeros_like(Di)\n",
    "\n",
    "        for _ in range(max_iter):\n",
    "            H, Pi = self._compute_entropy_and_conditional_probs(Di, sigma)\n",
    "            diff = H - target_entropy\n",
    "            if abs(diff) < tol:\n",
    "                break\n",
    "            if diff > 0:  # entropy too high ‚áí decrease sigma\n",
    "                sigma_max = sigma\n",
    "                sigma = (sigma + sigma_min) / 2 if np.isfinite(sigma_max) else sigma / 2\n",
    "            else:  # entropy too low ‚áí increase sigma\n",
    "                sigma_min = sigma\n",
    "                sigma = (sigma + sigma_max) / 2 if np.isfinite(sigma_max) else sigma * 2\n",
    "            sigma = max(sigma, 1e-5)\n",
    "\n",
    "        return sigma, Pi\n",
    "\n",
    "    def _compute_high_dim_affinities(self, X):\n",
    "        \"\"\"Compute symmetric affinity matrix P and per-point sigmas.\"\"\"\n",
    "        X = np.asarray(X, dtype=float)\n",
    "        n_samples = X.shape[0]\n",
    "        D = self._compute_pairwise_distances(X)\n",
    "        P_cond = np.zeros_like(D)\n",
    "        sigmas = np.zeros(n_samples)\n",
    "\n",
    "        for i in range(n_samples):\n",
    "            sigma, Pi = self._binary_search_sigma(D[i], self.perplexity)\n",
    "            sigmas[i] = sigma\n",
    "            P_cond[i] = Pi\n",
    "\n",
    "        P = self._symmetrize_probabilities(P_cond)\n",
    "        return P, sigmas\n",
    "\n",
    "    def _symmetrize_probabilities(self, P_cond):\n",
    "        \"\"\"Symmetrize conditional probabilities into joint distribution.\"\"\"\n",
    "        n = P_cond.shape[0]\n",
    "        P = (P_cond + P_cond.T) / (2.0 * n)\n",
    "        np.fill_diagonal(P, 0.0)\n",
    "        P = np.maximum(P, 1e-12)\n",
    "        P /= np.sum(P)\n",
    "        return P\n",
    "\n",
    "    def _compute_low_dim_affinities(self, Y):\n",
    "        \"\"\"Compute Student-t based low-dimensional affinities.\"\"\"\n",
    "        D = self._compute_pairwise_distances(Y)\n",
    "        inv = 1.0 / (1.0 + D)\n",
    "        np.fill_diagonal(inv, 0.0)\n",
    "        Q = inv / np.sum(inv)\n",
    "        Q = np.maximum(Q, 1e-12)\n",
    "        Q /= np.sum(Q)\n",
    "        return Q\n",
    "\n",
    "    def _compute_kl_divergence(self, P, Q):\n",
    "        \"\"\"Return KL(P‚ÄñQ).\"\"\"\n",
    "        mask = P > 0\n",
    "        return np.sum(P[mask] * (np.log(P[mask]) - np.log(Q[mask])))\n",
    "\n",
    "    def _compute_gradient(self, P, Q, Y):\n",
    "        \"\"\"Compute gradient of KL divergence with respect to Y.\"\"\"\n",
    "        distances = self._compute_pairwise_distances(Y)\n",
    "        inv = 1.0 / (1.0 + distances)\n",
    "        np.fill_diagonal(inv, 0.0)\n",
    "        coeff = 4.0 * ((P - Q) * inv)\n",
    "        grad = np.zeros_like(Y)\n",
    "        for i in range(Y.shape[0]):\n",
    "            diff = Y[i] - Y\n",
    "            grad[i] = np.sum(coeff[i][:, None] * diff, axis=0)\n",
    "        return grad\n",
    "\n",
    "    def _initialize_embedding(self, n_samples, random_state=None):\n",
    "        \"\"\"Randomly initialize the embedding.\"\"\"\n",
    "        rng = np.random.RandomState(random_state)\n",
    "        return rng.normal(loc=0.0, scale=1e-4, size=(n_samples, self.n_components))\n",
    "\n",
    "    def _optimize_embedding(self, P, Y_init, early_exaggeration=4.0, momentum=0.5):\n",
    "        \"\"\"Optimize embedding via gradient descent with momentum.\"\"\"\n",
    "        Y = Y_init.copy()\n",
    "        velocity = np.zeros_like(Y)\n",
    "        switch_iter = min(250, self.n_iter // 2)\n",
    "\n",
    "        for it in range(self.n_iter):\n",
    "            P_use = P * early_exaggeration if it < switch_iter else P\n",
    "            Q = self._compute_low_dim_affinities(Y)\n",
    "            grad = self._compute_gradient(P_use, Q, Y)\n",
    "            if it == switch_iter:\n",
    "                momentum = 0.8\n",
    "            velocity = momentum * velocity - self.learning_rate * grad\n",
    "            Y += velocity\n",
    "            Y -= np.mean(Y, axis=0, keepdims=True)\n",
    "        return Y\n",
    "\n",
    "    def _compute_distances_to_original(self, X_original, X_new):\n",
    "        \"\"\"Compute squared distances from new to original points.\"\"\"\n",
    "        X_original = np.asarray(X_original, dtype=float)\n",
    "        X_new = np.asarray(X_new, dtype=float)\n",
    "        sum_new = np.sum(X_new ** 2, axis=1, keepdims=True)\n",
    "        sum_orig = np.sum(X_original ** 2, axis=1)\n",
    "        D = sum_new + sum_orig - 2 * np.dot(X_new, X_original.T)\n",
    "        return np.maximum(D, 0)\n",
    "\n",
    "    def _compute_conditional_probs_new(self, X_original, X_new, sigmas_original=None):\n",
    "        \"\"\"Compute conditional probabilities from new points to original ones.\"\"\"\n",
    "        D_new = self._compute_distances_to_original(X_original, X_new)\n",
    "        P_new = np.zeros_like(D_new)\n",
    "        for i in range(D_new.shape[0]):\n",
    "            _, Pi = self._binary_search_sigma(D_new[i], self.perplexity)\n",
    "            P_new[i] = Pi\n",
    "        return P_new\n",
    "\n",
    "    def _optimize_new_point_embedding(self, Y_original, P_new, Y_new_init, n_iter=None):\n",
    "        \"\"\"Optimize embeddings for new points while keeping originals fixed.\"\"\"\n",
    "        Y_original = np.asarray(Y_original, dtype=float)\n",
    "        Y_new = Y_new_init.copy()\n",
    "        if n_iter is None:\n",
    "            n_iter = max(250, self.n_iter // 4)\n",
    "\n",
    "        for _ in range(n_iter):\n",
    "            for idx in range(Y_new.shape[0]):\n",
    "                diff = Y_new[idx] - Y_original\n",
    "                dist_sq = np.sum(diff ** 2, axis=1)\n",
    "                inv = 1.0 / (1.0 + dist_sq)\n",
    "                Q = inv / np.sum(inv)\n",
    "                coeff = 4.0 * (P_new[idx] - Q) * inv\n",
    "                grad = np.sum(coeff[:, None] * diff, axis=0)\n",
    "                Y_new[idx] -= self.learning_rate * grad\n",
    "            Y_new -= np.mean(Y_new, axis=0, keepdims=True)\n",
    "        return Y_new\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90722b4",
   "metadata": {},
   "source": [
    "## tests\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe0a8ad",
   "metadata": {},
   "source": [
    "First we will test all of our private methods:\n",
    "1. **_compute_pairwise_distances**\n",
    "2. **_compute_entropy_and_conditional_probs**\n",
    "3. **_binary_search_sigma**\n",
    "4. **_compute_high_dim_affinities**\n",
    "5. **_symmetrize_probabilities**\n",
    "6. **_compute_low_dim_affinities**\n",
    "7. **_compute_kl_divergence**\n",
    "8. **_compute_gradient**\n",
    "9. **_initialize_embedding**\n",
    "10. **_optimize_embedding**\n",
    "11. **_compute_distances_to_original**\n",
    "12. **_compute_conditional_probs_new**\n",
    "13. **_optimize_new_point_embedding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "38fcde6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ pairwise distances\n",
      "‚úÖ entropy & conditional probs\n",
      "‚úÖ binary search sigma\n",
      "‚úÖ symmetrize probabilities\n",
      "‚úÖ high-dim affinities\n",
      "‚úÖ low-dim affinities\n",
      "‚úÖ KL divergence\n",
      "‚úÖ gradient\n",
      "‚úÖ initialize embedding\n",
      "‚úÖ optimize embedding\n",
      "‚úÖ distances + P_new\n",
      "‚úÖ optimize new points\n",
      "\n",
      "====== SUMMARY ======\n",
      "Passed: 12\n",
      "Failed: 0\n",
      "=====================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 1. _compute_pairwise_distances\n",
    "# ============================================================\n",
    "def test_compute_pairwise_distances_basic():\n",
    "    cases = [\n",
    "        (np.array([[0.0], [1.0]]), \"1D two points distance 1\"),\n",
    "        (np.array([[0.0, 0.0], [3.0, 4.0]]), \"2D (0,0) and (3,4) distance 5^2=25\"),\n",
    "        (np.array([[1.0, 2.0], [1.0, 2.0]]), \"identical points distance 0\"),\n",
    "    ]\n",
    "    tsne = CustomTSNE()\n",
    "    for X, desc in cases:\n",
    "        D = tsne._compute_pairwise_distances(X)\n",
    "        n = X.shape[0]\n",
    "        assert D.shape == (n, n), f\"[{desc}] shape {D.shape} != {(n, n)}\"\n",
    "        assert np.all(D >= -1e-12), f\"[{desc}] negative distances, min={D.min()}\"\n",
    "        assert np.allclose(np.diag(D), 0.0), f\"[{desc}] diagonal not zero: {np.diag(D)}\"\n",
    "        assert np.allclose(D, D.T), f\"[{desc}] matrix is not symmetric\"\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 2. _compute_entropy_and_conditional_probs\n",
    "# ============================================================\n",
    "def test_compute_entropy_and_conditional_probs_properties():\n",
    "    cases = [\n",
    "        (np.array([0.0, 1.0, 4.0]), 1.0, \"simple distances, moderate sigma\"),\n",
    "        (np.array([0.0, 10.0, 20.0]), 0.1, \"very small sigma, sharp distribution\"),\n",
    "        (np.array([0.0, 1.0, 1.0]), 5.0, \"large sigma, more uniform\"),\n",
    "    ]\n",
    "    tsne = CustomTSNE()\n",
    "    for Di, sigma, desc in cases:\n",
    "        H, P = tsne._compute_entropy_and_conditional_probs(Di, sigma)\n",
    "        assert P.shape == Di.shape, f\"[{desc}] P shape {P.shape} != Di shape {Di.shape}\"\n",
    "        assert np.isfinite(H), f\"[{desc}] entropy not finite: {H}\"\n",
    "        assert np.all(P >= 0), f\"[{desc}] negative prob, min={P.min()}\"\n",
    "        assert np.isclose(P.sum(), 1.0, atol=1e-6), f\"[{desc}] P.sum={P.sum()}\"\n",
    "        assert P[0] <= 1e-10, f\"[{desc}] self prob (Di==0) must be 0, got {P[0]}\"\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 3. _binary_search_sigma\n",
    "# ============================================================\n",
    "def test_binary_search_sigma_matches_perplexity():\n",
    "    cases = [\n",
    "        (np.array([0.0, 1.0, 4.0, 9.0]), 2.0, \"target perplexity 2\"),\n",
    "        (np.array([0.0, 0.5, 2.0, 8.0, 15.0]), 4.0, \"target perplexity 4\"),\n",
    "        (np.array([0.0, 3.0, 6.0, 9.0, 12.0, 15.0, 20.0]), 6.0, \"target perplexity 6\"),\n",
    "    ]\n",
    "    tsne = CustomTSNE()\n",
    "    for Di, desired_perplexity, desc in cases:\n",
    "        sigma, Pi = tsne._binary_search_sigma(Di, desired_perplexity)\n",
    "        nz = Pi > 0\n",
    "        H = -np.sum(Pi[nz] * np.log2(Pi[nz]))\n",
    "        perp = 2 ** H\n",
    "        assert sigma > 0, f\"[{desc}] sigma <= 0: {sigma}\"\n",
    "        assert np.isclose(perp, desired_perplexity, rtol=1e-2), \\\n",
    "            f\"[{desc}] perp={perp}, expected {desired_perplexity}\"\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 4. _symmetrize_probabilities\n",
    "# ============================================================\n",
    "def test_symmetrize_probabilities_small_example():\n",
    "    tsne = CustomTSNE()\n",
    "    P_cond = np.array(\n",
    "        [\n",
    "            [0.0, 0.7, 0.3],\n",
    "            [0.6, 0.0, 0.4],\n",
    "            [0.2, 0.8, 0.0],\n",
    "        ]\n",
    "    )\n",
    "    P = tsne._symmetrize_probabilities(P_cond)\n",
    "    assert P.shape == P_cond.shape, \"P shape mismatch\"\n",
    "    assert np.all(P >= 0), f\"P has negative entries, min={P.min()}\"\n",
    "    assert np.allclose(P, P.T), \"P not symmetric\"\n",
    "    assert np.isclose(P.sum(), 1.0, atol=1e-6), f\"P.sum={P.sum()}\"\n",
    "    assert np.allclose(np.diag(P), 0.0), \"diag(P) not zero\"\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 5. _compute_high_dim_affinities\n",
    "# ============================================================\n",
    "def test_compute_high_dim_affinities_properties():\n",
    "    cases = [\n",
    "        (3, 2, \"3 points in 2D\"),\n",
    "        (5, 3, \"5 points in 3D\"),\n",
    "        (10, 4, \"10 points in 4D\"),\n",
    "    ]\n",
    "    for n_samples, n_features, desc in cases:\n",
    "        rng = np.random.RandomState(0)\n",
    "        X = rng.normal(size=(n_samples, n_features))\n",
    "        tsne = CustomTSNE(perplexity=3.0)\n",
    "        P, sigmas = tsne._compute_high_dim_affinities(X)\n",
    "\n",
    "        assert P.shape == (n_samples, n_samples), f\"[{desc}] P shape={P.shape}\"\n",
    "        assert sigmas.shape == (n_samples,), f\"[{desc}] sigmas shape={sigmas.shape}\"\n",
    "        assert np.all(sigmas > 0), f\"[{desc}] non-positive sigma: {sigmas}\"\n",
    "        assert np.isclose(P.sum(), 1.0, atol=1e-6), f\"[{desc}] P.sum={P.sum()}\"\n",
    "        assert np.allclose(P, P.T), f\"[{desc}] P not symmetric\"\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 6. _compute_low_dim_affinities\n",
    "# ============================================================\n",
    "def test_compute_low_dim_affinities_properties():\n",
    "    rng = np.random.RandomState(1)\n",
    "    cases = [\n",
    "        (np.array([[0.0, 0.0], [1.0, 0.0]]), \"two points in 2D\"),\n",
    "        (np.array([[0.0], [1.0], [2.0]]), \"three points in 1D\"),\n",
    "        (rng.normal(size=(4, 3)), \"four points in 3D\"),\n",
    "    ]\n",
    "    tsne = CustomTSNE()\n",
    "    for Y, desc in cases:\n",
    "        Q = tsne._compute_low_dim_affinities(Y)\n",
    "        n = Y.shape[0]\n",
    "        assert Q.shape == (n, n), f\"[{desc}] Q shape={Q.shape}\"\n",
    "        assert np.all(Q >= 0), f\"[{desc}] Q has negative entries, min={Q.min()}\"\n",
    "        assert np.isclose(Q.sum(), 1.0, atol=1e-6), f\"[{desc}] Q.sum={Q.sum()}\"\n",
    "        assert np.allclose(np.diag(Q), 0.0), f\"[{desc}] diag(Q) not zero\"\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 7. _compute_kl_divergence\n",
    "# ============================================================\n",
    "def test_compute_kl_divergence_zero_and_positive():\n",
    "    tsne = CustomTSNE()\n",
    "    P = np.array([[0.0, 0.5, 0.5],\n",
    "                  [0.5, 0.0, 0.5],\n",
    "                  [0.5, 0.5, 0.0]])\n",
    "    P = P / P.sum()\n",
    "\n",
    "    kl_same = tsne._compute_kl_divergence(P, P)\n",
    "    assert np.isclose(kl_same, 0.0, atol=1e-10), f\"KL(P||P)={kl_same} != 0\"\n",
    "\n",
    "    Q = P.copy()\n",
    "    Q[0, 1] += 0.1\n",
    "    Q[0, 2] -= 0.1\n",
    "    Q = np.maximum(Q, 1e-12)\n",
    "    Q /= Q.sum()\n",
    "    kl_diff = tsne._compute_kl_divergence(P, Q)\n",
    "    assert kl_diff > 0, f\"KL(P||Q) should be >0, got {kl_diff}\"\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 8. _compute_gradient\n",
    "# ============================================================\n",
    "def test_compute_gradient_shape_and_finiteness():\n",
    "    rng = np.random.RandomState(0)\n",
    "    X = rng.normal(size=(5, 2))\n",
    "    tsne = CustomTSNE(perplexity=3.0, n_components=2, n_iter=250)\n",
    "\n",
    "    P, _ = tsne._compute_high_dim_affinities(X)\n",
    "    Y_init = tsne._initialize_embedding(len(X), random_state=0)\n",
    "    Q = tsne._compute_low_dim_affinities(Y_init)\n",
    "    grad = tsne._compute_gradient(P, Q, Y_init)\n",
    "\n",
    "    assert grad.shape == Y_init.shape, f\"grad shape {grad.shape}, expected {Y_init.shape}\"\n",
    "    assert np.all(np.isfinite(grad)), \"grad has NaN or inf\"\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 9. _initialize_embedding\n",
    "# ============================================================\n",
    "def test_initialize_embedding_stats():\n",
    "    cases = [(3, 2), (10, 2), (5, 3)]\n",
    "    for n_samples, n_components in cases:\n",
    "        tsne = CustomTSNE(n_components=n_components)\n",
    "        Y = tsne._initialize_embedding(n_samples, random_state=123)\n",
    "        assert Y.shape == (n_samples, n_components), \"shape mismatch\"\n",
    "        mean = np.mean(Y)\n",
    "        std = np.std(Y)\n",
    "        assert abs(mean) < 1e-3, f\"mean={mean} not near 0\"\n",
    "        assert 1e-5 < std < 1e-3, f\"std={std} not around 1e-4\"\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 10. _optimize_embedding\n",
    "# ============================================================\n",
    "def test_optimize_embedding_non_worsening_kl():\n",
    "    rng = np.random.RandomState(0)\n",
    "    X = rng.normal(size=(8, 3))\n",
    "    tsne = CustomTSNE(perplexity=3.0, n_components=2, n_iter=100, learning_rate=100.0)\n",
    "\n",
    "    P, _ = tsne._compute_high_dim_affinities(X)\n",
    "    Y_init = tsne._initialize_embedding(len(X), random_state=0)\n",
    "    Q_init = tsne._compute_low_dim_affinities(Y_init)\n",
    "    kl_before = tsne._compute_kl_divergence(P, Q_init)\n",
    "\n",
    "    Y_opt = tsne._optimize_embedding(P, Y_init)\n",
    "    Q_opt = tsne._compute_low_dim_affinities(Y_opt)\n",
    "    kl_after = tsne._compute_kl_divergence(P, Q_opt)\n",
    "\n",
    "    assert kl_after <= kl_before * 1.2, \\\n",
    "        f\"KL increased too much: before={kl_before}, after={kl_after}\"\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 11. _compute_distances_to_original + _compute_conditional_probs_new\n",
    "# ============================================================\n",
    "def test_compute_distances_and_conditional_probs_new():\n",
    "    X_original = np.array([[0.0, 0.0], [1.0, 0.0], [0.0, 1.0]])\n",
    "    X_new = np.array([[0.5, 0.0], [0.0, 0.5]])\n",
    "\n",
    "    tsne = CustomTSNE(perplexity=2.0)\n",
    "    D_new = tsne._compute_distances_to_original(X_original, X_new)\n",
    "\n",
    "    d0 = (0.5 - 0.0) ** 2 + (0.0 - 0.0) ** 2\n",
    "    d1 = (0.5 - 1.0) ** 2 + (0.0 - 0.0) ** 2\n",
    "    d2 = (0.5 - 0.0) ** 2 + (0.0 - 1.0) ** 2\n",
    "\n",
    "    assert np.isclose(D_new[0, 0], d0), \"distance[0,0] mismatch\"\n",
    "    assert np.isclose(D_new[0, 1], d1), \"distance[0,1] mismatch\"\n",
    "    assert np.isclose(D_new[0, 2], d2), \"distance[0,2] mismatch\"\n",
    "\n",
    "    P_new = tsne._compute_conditional_probs_new(X_original, X_new)\n",
    "    assert P_new.shape == D_new.shape, \"P_new shape mismatch\"\n",
    "    row_sums = P_new.sum(axis=1)\n",
    "    assert np.allclose(row_sums, 1.0, atol=1e-6), f\"P_new rows sum={row_sums}\"\n",
    "    assert np.all(P_new >= 0), f\"P_new has negative entries, min={P_new.min()}\"\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 12. _optimize_new_point_embedding\n",
    "# ============================================================\n",
    "def test_optimize_new_point_embedding_converges_reasonably():\n",
    "    rng = np.random.RandomState(0)\n",
    "    X_orig = rng.normal(size=(10, 3))\n",
    "    tsne = CustomTSNE(perplexity=3.0, n_components=2, n_iter=200, learning_rate=50.0)\n",
    "\n",
    "    P_orig, _ = tsne._compute_high_dim_affinities(X_orig)\n",
    "    Y_init = tsne._initialize_embedding(len(X_orig), random_state=0)\n",
    "    Y_orig = tsne._optimize_embedding(P_orig, Y_init)\n",
    "\n",
    "    X_new = np.mean(X_orig, axis=0, keepdims=True) + rng.normal(scale=0.1, size=(3, 3))\n",
    "    P_new = tsne._compute_conditional_probs_new(X_orig, X_new)\n",
    "    Y_new_init = rng.normal(scale=1e-4, size=(3, tsne.n_components))\n",
    "    Y_new = tsne._optimize_new_point_embedding(Y_orig, P_new, Y_new_init, n_iter=100)\n",
    "\n",
    "    assert Y_new.shape == (3, tsne.n_components), \"Y_new shape mismatch\"\n",
    "    assert np.all(np.isfinite(Y_new)), \"Y_new has NaN or inf\"\n",
    "    orig_scale = np.std(Y_orig)\n",
    "    new_scale = np.std(Y_new)\n",
    "    assert new_scale < 10 * orig_scale, \\\n",
    "        f\"new variance {new_scale} too large vs {orig_scale}\"\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Simple mini-runner for the notebook\n",
    "# ============================================================\n",
    "def run_all_custom_tsne_tests():\n",
    "    tests = [\n",
    "        (\"pairwise distances\", test_compute_pairwise_distances_basic),\n",
    "        (\"entropy & conditional probs\", test_compute_entropy_and_conditional_probs_properties),\n",
    "        (\"binary search sigma\", test_binary_search_sigma_matches_perplexity),\n",
    "        (\"symmetrize probabilities\", test_symmetrize_probabilities_small_example),\n",
    "        (\"high-dim affinities\", test_compute_high_dim_affinities_properties),\n",
    "        (\"low-dim affinities\", test_compute_low_dim_affinities_properties),\n",
    "        (\"KL divergence\", test_compute_kl_divergence_zero_and_positive),\n",
    "        (\"gradient\", test_compute_gradient_shape_and_finiteness),\n",
    "        (\"initialize embedding\", test_initialize_embedding_stats),\n",
    "        (\"optimize embedding\", test_optimize_embedding_non_worsening_kl),\n",
    "        (\"distances + P_new\", test_compute_distances_and_conditional_probs_new),\n",
    "        (\"optimize new points\", test_optimize_new_point_embedding_converges_reasonably),\n",
    "    ]\n",
    "\n",
    "    n_pass, n_fail = 0, 0\n",
    "    for name, func in tests:\n",
    "        try:\n",
    "            func()\n",
    "            print(f\"‚úÖ {name}\")\n",
    "            n_pass += 1\n",
    "        except AssertionError as e:\n",
    "            print(f\"‚ùå {name} ‚Äî ASSERTION FAILED: {e}\")\n",
    "            n_fail += 1\n",
    "        except Exception as e:\n",
    "            print(f\"üí• {name} ‚Äî ERROR: {type(e).__name__}: {e}\")\n",
    "            n_fail += 1\n",
    "\n",
    "    print(\"\\n====== SUMMARY ======\")\n",
    "    print(f\"Passed: {n_pass}\")\n",
    "    print(f\"Failed: {n_fail}\")\n",
    "    print(\"=====================\")\n",
    "\n",
    "\n",
    "# actually run them now\n",
    "run_all_custom_tsne_tests()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b552a0",
   "metadata": {},
   "source": [
    "Now we will test the main `fit_transform()` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8a14559b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ fit_transform basic\n",
      "‚úÖ fit_transform invalid input\n",
      "‚úÖ fit_transform determinism\n",
      "\n",
      "====== SUMMARY ======\n",
      "Passed: 3\n",
      "Failed: 0\n",
      "=====================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 13. fit_transform ‚Äì basic properties & state\n",
    "# ============================================================\n",
    "def test_fit_transform_basic_properties():\n",
    "    rng = np.random.RandomState(0)\n",
    "    n_samples, n_features, n_components = 20, 5, 2\n",
    "    X = rng.normal(size=(n_samples, n_features))\n",
    "\n",
    "    tsne = CustomTSNE(perplexity=5.0, n_components=n_components, n_iter=250, learning_rate=100.0)\n",
    "    Y = tsne.fit_transform(X)\n",
    "\n",
    "    # 1) Output shape and finiteness\n",
    "    assert Y.shape == (n_samples, n_components), f\"Y shape {Y.shape}, expected {(n_samples, n_components)}\"\n",
    "    assert np.all(np.isfinite(Y)), \"Y has NaN or inf\"\n",
    "\n",
    "    # 2) Internal state attributes\n",
    "    assert tsne.X_fit_ is not None, \"X_fit_ not set\"\n",
    "    assert tsne.sigmas_ is not None, \"sigmas_ not set\"\n",
    "    assert tsne.Y_fit_ is not None, \"Y_fit_ not set\"\n",
    "\n",
    "    assert tsne.X_fit_.shape == X.shape, f\"X_fit_.shape {tsne.X_fit_.shape}, expected {X.shape}\"\n",
    "    assert tsne.sigmas_.shape == (n_samples,), f\"sigmas_.shape {tsne.sigmas_.shape}, expected {(n_samples,)}\"\n",
    "    assert tsne.Y_fit_.shape == Y.shape, f\"Y_fit_.shape {tsne.Y_fit_.shape}, expected {Y.shape}\"\n",
    "\n",
    "    # 3) sigmas positive\n",
    "    assert np.all(tsne.sigmas_ > 0), f\"non-positive sigmas: {tsne.sigmas_}\"\n",
    "\n",
    "    # 4) stored embedding matches returned embedding\n",
    "    assert np.allclose(tsne.Y_fit_, Y), \"Y_fit_ and returned Y differ\"\n",
    "\n",
    "    # 5) embedding roughly centered\n",
    "    mean = Y.mean(axis=0)\n",
    "    assert np.allclose(mean, 0.0, atol=1e-5), f\"embedding not centered, mean={mean}\"\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 14. fit_transform ‚Äì input validation\n",
    "# ============================================================\n",
    "def test_fit_transform_raises_on_invalid_input():\n",
    "    tsne = CustomTSNE()\n",
    "\n",
    "    # 1D array -> must raise\n",
    "    X_1d = np.array([1.0, 2.0, 3.0])\n",
    "    try:\n",
    "        tsne.fit_transform(X_1d)\n",
    "        assert False, \"Expected ValueError for 1D input, but none was raised\"\n",
    "    except ValueError:\n",
    "        pass\n",
    "\n",
    "    # 3D array -> must raise\n",
    "    X_3d = np.zeros((2, 2, 2))\n",
    "    try:\n",
    "        tsne.fit_transform(X_3d)\n",
    "        assert False, \"Expected ValueError for 3D input, but none was raised\"\n",
    "    except ValueError:\n",
    "        pass\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 15. fit_transform ‚Äì determinism for same data & params\n",
    "# ============================================================\n",
    "def test_fit_transform_deterministic():\n",
    "    rng = np.random.RandomState(123)\n",
    "    X = rng.normal(size=(15, 4))\n",
    "\n",
    "    tsne1 = CustomTSNE(perplexity=5.0, n_components=2, n_iter=200, learning_rate=100.0)\n",
    "    tsne2 = CustomTSNE(perplexity=5.0, n_components=2, n_iter=200, learning_rate=100.0)\n",
    "\n",
    "    Y1 = tsne1.fit_transform(X)\n",
    "    Y2 = tsne2.fit_transform(X)\n",
    "\n",
    "    # Because initialization uses fixed random_state=42, results should match\n",
    "    assert np.allclose(Y1, Y2), \"fit_transform is not deterministic for same data and params\"\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Simple mini-runner for the notebook (add to existing list)\n",
    "# ============================================================\n",
    "def run_all_custom_tsne_tests():\n",
    "    tests = [\n",
    "        (\"fit_transform basic\", test_fit_transform_basic_properties),\n",
    "        (\"fit_transform invalid input\", test_fit_transform_raises_on_invalid_input),\n",
    "        (\"fit_transform determinism\", test_fit_transform_deterministic),\n",
    "    ]\n",
    "\n",
    "    n_pass, n_fail = 0, 0\n",
    "    for name, func in tests:\n",
    "        try:\n",
    "            func()\n",
    "            print(f\"‚úÖ {name}\")\n",
    "            n_pass += 1\n",
    "        except AssertionError as e:\n",
    "            print(f\"‚ùå {name} ‚Äî ASSERTION FAILED: {e}\")\n",
    "            n_fail += 1\n",
    "        except Exception as e:\n",
    "            print(f\"üí• {name} ‚Äî ERROR: {type(e).__name__}: {e}\")\n",
    "            n_fail += 1\n",
    "\n",
    "    print(\"\\n====== SUMMARY ======\")\n",
    "    print(f\"Passed: {n_pass}\")\n",
    "    print(f\"Failed: {n_fail}\")\n",
    "    print(\"=====================\")\n",
    "\n",
    "\n",
    "# actually run them now\n",
    "run_all_custom_tsne_tests()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df24f179351fa008",
   "metadata": {
    "collapsed": false,
    "id": "df24f179351fa008",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Load data\n",
    "Please use the cell below to discuss your dataset choice and why it is appropriate (or not) for this algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c4083f-5267-44d3-89ed-65864f82aa57",
   "metadata": {
    "id": "74c4083f-5267-44d3-89ed-65864f82aa57"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14a3b8890e86f9",
   "metadata": {
    "id": "a14a3b8890e86f9"
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "\n",
    "# Normalize data if necessary\n",
    "\n",
    "# Split the data into train and test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da49bb42f79a55f",
   "metadata": {
    "collapsed": false,
    "id": "da49bb42f79a55f",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# t-SNE demonstration\n",
    "Demonstrate your t-SNE implementation.\n",
    "\n",
    "Add plots and figures. The code below is just to help you get started, and should not be your final submission.\n",
    "\n",
    "Please use the cell below to describe your results and tests.\n",
    "\n",
    "Describe the difference between your implementation and the sklearn implementation. Hint: you can look at the documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a064afb5-aeea-48d8-b315-921bf4f8238f",
   "metadata": {
    "id": "a064afb5-aeea-48d8-b315-921bf4f8238f"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3628856e1335fd",
   "metadata": {
    "id": "9b3628856e1335fd"
   },
   "outputs": [],
   "source": [
    "# Run your custom t-SNE implementation\n",
    "custom_tsne = CustomTSNE(n_components=2, perplexity=N/10)\n",
    "custom_Y = custom_tsne.fit_transform(X_train)\n",
    "\n",
    "# Run sklearn t-SNE\n",
    "sk_tsne = TSNE(n_components=2, init='random', perplexity=N/10)\n",
    "sk_Y = sk_tsne.fit_transform(X_train)\n",
    "\n",
    "# Visualization of the result\n",
    "plt.figure()\n",
    "plt.scatter(custom_Y[:, 0], custom_Y[:, 1], s=5, c=label_train.astype(int), cmap='tab10')\n",
    "plt.scatter(custom_Y[:, 0], custom_Y[:, 1], s=5, c=label_train.astype(int), cmap='tab10')\n",
    "plt.colorbar()\n",
    "plt.title('MNIST Data Embedded into 2D with Custom t-SNE')\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(sk_Y[:, 0], sk_Y[:, 1], s=5, c=label_train.astype(int), cmap='tab10')\n",
    "plt.colorbar()\n",
    "plt.title('MNIST Data Embedded into 2D with sklearn t-SNE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3527c23f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "73fa2fceedc77e92",
   "metadata": {
    "collapsed": false,
    "id": "73fa2fceedc77e92",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# t-SNE extension - mapping new samples\n",
    "Demonstrate your t-SNE transformation procedure.\n",
    "\n",
    "Add plots and figures.\n",
    "\n",
    "Please use the cell below t describe your suggested approach in detail. Use formal notations where appropriate.\n",
    "Describe and discuss your results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b34701c-cc3b-439a-b2d0-393449cf5a20",
   "metadata": {
    "id": "7b34701c-cc3b-439a-b2d0-393449cf5a20"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d38dc132b23e7b",
   "metadata": {
    "id": "9d38dc132b23e7b"
   },
   "outputs": [],
   "source": [
    "# Transform new data\n",
    "custom_Y_new = custom_tsne.transform(X_train,custom_Y,X_test)\n",
    "\n",
    "# Visualization of the result\n",
    "plt.figure()\n",
    "plt.scatter(custom_Y[:, 0], custom_Y[:, 1], s=5, c=label_train.astype(int), cmap='tab10')\n",
    "plt.scatter(custom_Y_new[:, 0], custom_Y_new[:, 1], marker = '*', s=50, linewidths=0.5, edgecolors='k', c=label_test.astype(int), cmap='tab10')\n",
    "plt.colorbar()\n",
    "plt.title('MNIST Data Embedded into 2D with Custom t-SNE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38fa85cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "18c95c7f-d3a9-4e3d-b539-02e020358766",
   "metadata": {
    "id": "18c95c7f-d3a9-4e3d-b539-02e020358766"
   },
   "source": [
    "# Use of generative AI\n",
    "Please use the cell below to describe your use of generative AI in this assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36753fd7-8b2d-487b-82ae-dc318eca3ca6",
   "metadata": {
    "id": "36753fd7-8b2d-487b-82ae-dc318eca3ca6"
   },
   "source": [
    "### Gen-AI Usage Summary\n",
    "\n",
    "For this assignment I used three Gen-AI tools :\n",
    "\n",
    "- **ChatGPT Pro** ‚Äì for general conceptual questions, clarifications, phrasing, LaTex convertion, simplle synax questions.  \n",
    "- **Gemini Pro (Learning Mode)** ‚Äì used only at the beginning to verify that I understood the presentation and algorithm step by step.  \n",
    "- **Cursor** ‚Äì used extensively for creating templates, docuemantation, refactors and writing tests.\n",
    "\n",
    "With **Cursor**, I explicitly structured the work so that each section was developed and tested separately. I asked targeted, limited questions (e.g., *‚ÄúExplain step X‚Äù*, *‚ÄúHelp me validate Y‚Äù*, *‚ÄúRewrite this specific function‚Äù*, *\"give me template for function that does Z\"*), and never asked it to produce a full end-to-end solution. All implementation decisions, testing, debugging, and integration were done by me.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5dd0beb-1df3-43c8-bf51-dee41b88c8c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "advanced-ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
